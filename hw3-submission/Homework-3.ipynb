{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Hotel Ratings on Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will focus on practicing two techniques: web scraping and regression. For the first part, we will get some basic information for each hotel in Boston. Then, we will fit a regression model on this information and try to analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1 (30 pts)**\n",
    "\n",
    "We will scrape the data using Beautiful Soup. For each hotel that our search returns, we will get the information below.\n",
    "\n",
    "![Information to be scraped](hotel_info.png)\n",
    "\n",
    "Of course, feel free to collect even more data if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code block is the function which returns a string with the ratings given by each review for each page\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "def page_scores(page_url, user_agent, headers):\n",
    "    page_review_string = \"\"\n",
    "    \n",
    "    #get html\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text.encode('utf-8')\n",
    "    \n",
    "    #put soup on the stove\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    #find all reviews on page and prepare to extract urls\n",
    "    reviews_onpage = soup.find_all(\"div\", {\"class\": \"reviewSelector\"})\n",
    "    review_urls = []\n",
    "    #print(reviews_onpage)\n",
    "    #extract urls from each review and get a list of review urls\n",
    "    for review in reviews_onpage:\n",
    "        review_url = review.find('a', href = True)\n",
    "        review_id = review['id']\n",
    "        if str(type(review_url)) != \"<type 'NoneType'>\":\n",
    "            review_url = review_url['href']\n",
    "            curr_review = {'id' : review_id, 'url' : review_url}\n",
    "            review_urls.append(curr_review)\n",
    "\n",
    "     \n",
    "    problem_reviews = 0\n",
    "    #begin processing reviews\n",
    "    for url in review_urls:\n",
    "        #construct url and request html\n",
    "        curr_review_url = base_url + url['url']\n",
    "        response = requests.get(curr_review_url, headers = headers)\n",
    "        html = response.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(html)\n",
    "    \n",
    "        highlight_review = soup.find(\"div\", {\"id\" : url['id']})\n",
    "        #print(str(type(highlight_review)))\n",
    "        if str(type(highlight_review)) != \"<type 'NoneType'>\":\n",
    "            quality_ratings = highlight_review.find_all(\"li\", {\"class\": \"recommend-answer\"})\n",
    "            for rating in quality_ratings:\n",
    "                description = rating.find(\"div\", {\"class\": \"recommend-description\"})\n",
    "                description = description.find(text = True)\n",
    "                score = rating.find(\"img\")\n",
    "                score = score['alt']\n",
    "        \n",
    "                rating_tuple = {\"review id\" : url['id'], \"description\" : description, \"score\" : score}\n",
    "                page_review_string = page_review_string + (rating_tuple['review id'] + \":\" + rating_tuple['description'] + \":\" + rating_tuple['score'][0] + \"\\n\")\n",
    "                \n",
    "        else:\n",
    "            problem_reviews = problem_reviews + 1\n",
    "\n",
    "    return(page_review_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#here we basically call the above function for all pages\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\"\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "base_url = \"http://www.tripadvisor.com\"\n",
    "rev_pg1 = \"/Hotel_Review-g60745-d89599-Reviews-Omni_Parker_House-Boston_Massachusetts.html\"\n",
    "cum_scores = []\n",
    "review_url = base_url + rev_pg1\n",
    "\n",
    "#find number of pages\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "response = requests.get(review_url, headers=headers)\n",
    "htmlpagess = response.text.encode('utf-8')\n",
    "soup = BeautifulSoup(htmlpagess)\n",
    "qwertyu = soup.find(\"a\", {\"class\": \"more taLnk\"})\n",
    "qwertyu = qwertyu.find(text=True)\n",
    "qwertyu = (str(qwertyu)[:5]).replace(',', '')\n",
    "qwertyu = int(qwertyu)\n",
    "qwertyu = math.ceil(qwertyu/10.)\n",
    "qwertyu = qwertyu*10-10\n",
    "\n",
    "print(\"We are scraping \"+str(qwertyu)+\" \"+\"Reviews\")\n",
    " \n",
    "\n",
    "#collect 1st page reviews\n",
    "output_string = page_scores(review_url, user_agent, headers)\n",
    "\n",
    "#while loop for every page w/ reviews\n",
    "_or_value = 10\n",
    "head = \"https://www.tripadvisor.com/Hotel_Review-g60745-d89599-Reviews-\"\n",
    "tail = \"-Omni_Parker_House-Boston_Massachusetts.html#REVIEWS\"\n",
    "while (_or_value <= qwertyu):\n",
    "    _or = \"or\" + str(_or_value)\n",
    "    reviewpg_url = head + _or + tail\n",
    "    #print(reviewpg_url)\n",
    "    output_string = output_string + page_scores(reviewpg_url, user_agent, headers)\n",
    "    _or_value = _or_value + 10\n",
    "    \n",
    "omniParkerData = open(\"OmniParkerData.txt\", \"w\")\n",
    "omniParkerData.write(output_string)\n",
    "omniParkerData.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2 (20 pts) **\n",
    "\n",
    "Now, we will use regression to analyze this information. First, we will fit a linear regression model that predicts the average rating. For example, for the hotel above, the average rating is\n",
    "\n",
    "$$ \\text{AVG_SCORE} = \\frac{1*31 + 2*33 + 3*98 + 4*504 + 5*1861}{2527}$$\n",
    "\n",
    "Use the model to analyze the important factors that decide the $\\text{AVG_SCORE}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query URL is: http://www.tripadvisor.com/TypeAheadJson?query=Boston%20Massachusetts&action=API\n",
      "Tourism URL is: http://www.tripadvisor.com/Tourism-g60745-Boston_Massachusetts-Vacations.html\n",
      "City URL is: http://www.tripadvisor.com/Hotels-g60745-Boston_Massachusetts-Hotels.html\n"
     ]
    }
   ],
   "source": [
    "#In this block of code we start from the TripAdvisor homepage and move to the url where all hotels are listed\n",
    "base_url = \"http://www.tripadvisor.com\"\n",
    "city = \"Boston\"\n",
    "state = \"Massachusetts\"\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\"\n",
    "\n",
    "\n",
    "url = \"%s/TypeAheadJson?query=%s%%20%s&action=API\" % (base_url, \"%20\".join(city.split()), state)\n",
    "print(\"Query URL is: \" + url)\n",
    "\n",
    "\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.text.encode('utf-8')\n",
    "\n",
    "js = json.loads(html)\n",
    "results = js['results']\n",
    "urls = results[0]['urls'][0]\n",
    "\n",
    "tourism_url = urls['url']\n",
    "\n",
    "url = base_url + tourism_url\n",
    "print(\"Tourism URL is: \" + url)\n",
    "\n",
    "\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "li = soup.find(\"li\", {\"class\": \"hotels twoLines\"})\n",
    "city_url = li.find('a', href = True)\n",
    "city_url = city_url['href']\n",
    "\n",
    "url = base_url + city_url\n",
    "url_ = base_url + city_url\n",
    "print(\"City URL is: \" + url)\n",
    "\n",
    "# Request hotel list\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.94444444  2.8125      4.28979144  3.75505051  3.71868132  4.49224405\n",
      "  4.11976048  4.70795215  3.59597156  3.99111506  4.26571251  3.24435591\n",
      "  3.95785877  4.21192053  4.14705882  4.43744752  3.375       3.1120332\n",
      "  3.74216867  4.31570109  3.14767442  3.89578164  4.59060403  3.74860335\n",
      "  2.75095785  3.79797048  4.02808989  4.19866667  4.41067538  3.99120603\n",
      "  4.37229862  4.67977528  3.97922438  4.0893015   3.99315403  3.99160369\n",
      "  4.01030503  4.74166667  3.86068702  3.79746835  4.00592593  4.73949099\n",
      "  4.01058425  4.16468378  4.46825718  4.64880734  4.49704724  4.63934426\n",
      "  4.57430341  3.00928793  4.11212815  4.53989232  4.2559415   3.03321678\n",
      "  4.16330451  4.62659574  4.62048193  4.39694656  4.39057816  4.67606043\n",
      "  4.0803653   4.11206897  3.22164948  4.16077441  4.42640364  4.30708661\n",
      "  4.69756098  4.60043668  4.07092199  4.39403553  4.20723363  3.70901844\n",
      "  4.30220713  4.56642336  4.0903584   4.23823529  4.24935065  3.90072799\n",
      "  4.62364425  4.57709251]\n"
     ]
    }
   ],
   "source": [
    "#In this block of code we compute the 80x1 vector for Y where each entry is the avg rating of a hotel, from all its ratings on the site\n",
    "import copy\n",
    "Y = []\n",
    "def getHotelURLOnePage (htmlpage,w):\n",
    "    soup = BeautifulSoup(htmlpage)\n",
    "    div = soup.find_all('div', {\"class\": \"listing easyClear  p13n_imperfect \"})\n",
    "    for i in div:\n",
    "        wert = i.find(\"a\", href = True )\n",
    "        mane = i.find('div', {'class': 'listing_title'})\n",
    "        wert = wert['href']\n",
    "        name = mane.find(text=True)\n",
    "        if name != \"Omni Parker House\" and name!=\"Element Boston Seaport\" :\n",
    "            w.append([str(name),base_url + wert])\n",
    "    return(w)\n",
    "    \n",
    "\n",
    "def getHotelURLAllPages (url, w):\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    htmlpage = response.text.encode('utf-8')\n",
    "    soup = BeautifulSoup(htmlpage)\n",
    "    div = soup.find('div', {'class': 'unified pagination standard_pagination'})\n",
    "    if div.find(\"span\", {'class': 'nav next ui_button disabled'}) != None:\n",
    "        w = getHotelURLOnePage(htmlpage,w)\n",
    "    elif div.find('span', {'class': 'nav previous ui_button disabled'})!=None:\n",
    "        w = getHotelURLOnePage(htmlpage,w)\n",
    "        urlmen = div.find('a', href = True)\n",
    "        urlmen = urlmen['href']\n",
    "        getHotelURLAllPages(base_url+urlmen,w)\n",
    "    else:\n",
    "        w = getHotelURLOnePage(htmlpage,w)\n",
    "        urlmen = div.find_all('a', href = True)\n",
    "        urlmen = urlmen[1]['href']\n",
    "        getHotelURLAllPages(base_url+urlmen,w)\n",
    "    return (w)    \n",
    "Y = getHotelURLAllPages(url,Y)\n",
    "y=copy.deepcopy(Y)\n",
    "\n",
    "def getAvgRating(w):\n",
    "    q=0\n",
    "    for k in w:\n",
    "        headers = { 'User-Agent' : user_agent }\n",
    "        response = requests.get(k[1], headers=headers)\n",
    "        htmlpage = response.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(htmlpage)\n",
    "        five = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_5'})\n",
    "        #print(five)\n",
    "        fifty = five.find_all('span')\n",
    "        fifty = fifty[2]\n",
    "        fifty = fifty.find(text=True)\n",
    "        #print (fifty)\n",
    "        four = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_4'})\n",
    "        forty = four.find_all('span')\n",
    "        forty = forty[2]\n",
    "        forty = forty.find(text=True)\n",
    "        #print (forty)\n",
    "        three = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_3'})\n",
    "        thirty = three.find_all('span')\n",
    "        thirty = thirty[2]\n",
    "        thirty = thirty.find(text=True)\n",
    "        #print (thirty)\n",
    "        two = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_2'})\n",
    "        twenty = two.find_all('span')\n",
    "        twenty=twenty[2]\n",
    "        twenty = twenty.find(text=True)\n",
    "        #print(twenty)\n",
    "        one = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_1'})\n",
    "        ten = one.find_all('span')\n",
    "        ten = ten[2]\n",
    "        ten = ten.find(text=True)\n",
    "        #print(ten)\n",
    "        if fifty == None:\n",
    "            fifty = \"0\"\n",
    "        if forty == None:\n",
    "            forty = \"0\"\n",
    "        if thirty == None:\n",
    "            thirty = '0'\n",
    "        if twenty == None:\n",
    "            twenty = '0'\n",
    "        if ten == None:\n",
    "            ten = '0'\n",
    "            \n",
    "        w[q][1] = float(int(fifty.replace(',', ''))*5+int(forty.replace(',', ''))*4+int(thirty.replace(',', ''))*3+int(twenty.replace(',', ''))*2+int(ten.replace(',', ''))*1)/float(int(fifty.replace(',', ''))+int(forty.replace(',', ''))+int(thirty.replace(',', ''))+int(twenty.replace(',', ''))+int(ten.replace(',', '')))\n",
    "        q=q+1\n",
    "    w = sorted(w, key=lambda tup: tup[0])\n",
    "    w = np.array(w)\n",
    "    return(w[:,1].astype(float))\n",
    "Y = getAvgRating(Y)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.16666667  4.25        3.8         3.90909091  4.5         4.        ]\n",
      " [ 2.76923077  2.63636364  2.71428571  2.42857143  3.          2.85714286]\n",
      " [ 4.63311258  4.76090014  4.41369472  4.37988827  4.25782414  4.02745098]\n",
      " [ 4.18502825  3.26190476  4.07725948  4.10375     3.92924528  3.96923077]\n",
      " [ 3.87341772  4.05221932  3.67277487  4.10368664  3.98507463  3.76214834]\n",
      " [ 4.72972973  4.59568733  4.56326531  4.50222717  4.57957958  4.04587156]\n",
      " [ 4.38461538  4.80357143  3.86842105  4.23809524  3.95652174  3.73170732]\n",
      " [ 4.82427536  4.81818182  4.70103093  4.8001514   4.7102593   4.26526892]\n",
      " [ 3.81545064  4.44325153  3.42733813  3.89114833  3.85267035  3.73691655]\n",
      " [ 4.38624339  4.71909425  4.04053237  4.12209302  4.20926641  3.67706528]\n",
      " [ 4.49683258  4.85958254  4.18615529  4.37795276  4.26860841  3.75883953]\n",
      " [ 3.71475641  4.58318651  3.13330066  3.60331606  3.6137163   3.25142857]\n",
      " [ 4.33635188  4.40308989  3.74578652  4.17420814  3.99236641  3.99615877]\n",
      " [ 4.50897227  4.82183908  3.879046    4.24137931  4.10174419  4.18512658]\n",
      " [ 4.41945607  4.66337719  3.97039474  4.17064545  4.17493113  4.11839323]\n",
      " [ 4.57831978  4.76848138  4.41911765  4.52570407  4.56038961  4.18353068]\n",
      " [ 3.76264591  3.14767932  3.46694215  3.62893082  3.53439153  3.45945946]\n",
      " [ 3.44736842  3.72554348  3.10511364  3.44866071  3.41059603  3.61803714]\n",
      " [ 3.74358974  4.54983923  3.51369863  3.81021898  3.79591837  3.89028213]\n",
      " [ 4.60457516  4.81594373  4.13151927  4.50275229  4.37515684  4.10315789]\n",
      " [ 3.63636364  4.14516129  2.86996904  3.72962484  3.20886076  3.11773472]\n",
      " [ 4.36805556  3.52592593  4.25        4.03966006  4.19138756  4.00719424]\n",
      " [ 4.68314607  4.84447145  4.47677262  4.6925      4.55274262  4.32178771]\n",
      " [ 4.12105263  3.92761394  3.8292011   3.90064795  3.903125    3.53989362]\n",
      " [ 2.92376682  3.4354067   2.84888889  3.14830508  3.12972973  2.97787611]\n",
      " [ 4.19399786  3.61060948  3.96602492  4.05047619  4.075       3.7655398 ]\n",
      " [ 4.36878049  3.90899899  4.24633431  4.26068003  4.20962199  3.92023346]\n",
      " [ 4.60669456  4.39591837  4.38477801  4.36609687  4.35652174  3.98121086]\n",
      " [ 4.56200942  4.73211314  4.36867863  4.53388823  4.4939759   4.01843318]\n",
      " [ 4.3254717   4.21260504  4.19423868  4.14918256  4.25673077  3.9216152 ]\n",
      " [ 4.55092593  4.84833795  4.2662116   4.47100656  4.44444444  3.88925295]\n",
      " [ 4.84778013  4.89766407  4.62046205  4.77068966  4.78743961  4.1652452 ]\n",
      " [ 4.49672131  3.57624113  4.28833333  4.2962963   4.05708661  4.06504065]\n",
      " [ 4.44482366  4.8196319   4.1092832   4.064       4.24960754  4.08988764]\n",
      " [ 4.36111111  4.59831278  4.07400257  4.14307932  4.21005251  3.7448534 ]\n",
      " [ 4.37456243  4.6276958   4.06836616  4.16835327  4.16542289  3.67599068]\n",
      " [ 4.38012015  4.6018412   4.19272314  4.07664066  4.30032573  3.61031833]\n",
      " [ 4.87610619  4.65486726  4.80769231  4.72067039  4.75700935  4.57692308]\n",
      " [ 4.11344538  3.64772727  3.80743982  4.07387387  4.00294985  3.92402464]\n",
      " [ 4.00374532  4.32945736  3.63117871  4.31197772  3.68807339  3.66666667]\n",
      " [ 4.36775819  4.72672929  3.66494845  4.26029412  4.02887392  4.05196982]\n",
      " [ 4.87243402  4.76522085  4.79105628  4.80468269  4.75031579  4.4091076 ]\n",
      " [ 4.33976638  4.52622951  4.09003729  4.18671454  4.20908525  3.80784913]\n",
      " [ 4.49879904  4.38369099  4.29145729  4.35204435  4.3314121   3.87117552]\n",
      " [ 4.73728814  4.57415991  4.60618679  4.42568478  4.48265896  4.04161916]\n",
      " [ 4.76304156  4.89403341  4.50825861  4.75676736  4.588       4.37756011]\n",
      " [ 4.68879056  4.73235294  4.42205882  4.61931188  4.53606557  4.15820896]\n",
      " [ 4.85786802  4.87628866  4.75064935  4.69264069  4.75652174  4.11959288]\n",
      " [ 4.67008197  4.87727273  4.62882096  4.6         4.46239554  4.26124197]\n",
      " [ 3.49836066  4.32291667  2.73202614  3.328       3.31330472  3.29677419]\n",
      " [ 4.45753425  4.84210526  4.07880435  4.09480813  4.28174603  4.09893048]\n",
      " [ 4.68150032  4.82685978  4.48531375  4.61481481  4.58469055  4.16888889]\n",
      " [ 4.50180941  4.3111413   4.03396226  4.47484909  4.34991708  3.92102066]\n",
      " [ 3.34162679  3.2734375   3.08551992  3.45178571  3.29559748  3.22583732]\n",
      " [ 4.65284974  4.05858311  4.30232558  4.29265659  4.34326019  3.80653595]\n",
      " [ 4.79857398  4.76048951  4.61359571  4.67236842  4.56813417  4.24742268]\n",
      " [ 4.8597561   4.60227273  4.77837838  4.64980545  4.51592357  4.25294118]\n",
      " [ 4.55125285  4.425       4.38738739  4.54878049  4.44505495  4.18669528]\n",
      " [ 4.59822485  4.72711058  4.4941967   4.48571429  4.42622951  4.15246098]\n",
      " [ 4.82704626  4.604       4.67322404  4.73421301  4.71182365  4.33010405]\n",
      " [ 4.36428571  4.61577608  4.04522613  4.19474836  4.28647215  3.84064665]\n",
      " [ 4.40093897  4.84466019  4.12160804  4.27781983  4.37676439  3.84894837]\n",
      " [ 3.36046512  4.50098619  2.90856031  3.65335463  3.67102397  3.45155039]\n",
      " [ 4.47681332  4.84910486  4.25030377  4.26510721  4.23655914  3.85780886]\n",
      " [ 4.63609467  4.59235669  4.18902439  4.53244275  4.40569395  4.18769231]\n",
      " [ 4.73529412  4.6         4.5         4.45555556  4.20512821  3.86666667]\n",
      " [ 4.91964286  4.87826087  4.66055046  4.81395349  4.70642202  4.54954955]\n",
      " [ 4.59090909  4.84398977  4.64675325  4.40420561  4.46712803  4.40149626]\n",
      " [ 4.19392917  4.54561404  3.91485507  4.26875853  4.08775982  3.88888889]\n",
      " [ 4.70983607  4.60637382  4.42078365  4.54539202  4.52947154  4.03562552]\n",
      " [ 4.49180328  4.50162338  4.19933555  4.29919448  4.21892393  3.77969019]\n",
      " [ 3.96469939  4.5483504   3.55605381  4.0785      3.88108441  3.97857143]\n",
      " [ 4.6440281   4.53904282  4.3490099   4.45129225  4.55287009  3.83488372]\n",
      " [ 4.70048309  4.62121212  4.57009346  4.64317181  4.46625767  4.3254717 ]\n",
      " [ 4.47901399  3.7037037   4.22009908  4.210381    4.44842285  3.7147651 ]\n",
      " [ 4.44300647  4.80615551  4.29099427  4.27933884  4.49745547  3.86347607]\n",
      " [ 4.58230958  4.51764706  4.26098191  4.35742972  4.26278409  3.75405743]\n",
      " [ 4.24015444  4.5228505   3.99921936  4.0604782   4.12570056  3.66078281]\n",
      " [ 4.78059701  4.84818482  4.69278997  4.70698254  4.73417722  4.19330289]\n",
      " [ 4.84615385  4.15        4.65436242  4.72235872  4.56227758  4.61589404]]\n"
     ]
    }
   ],
   "source": [
    "#In this code block we compute the 80x6 X matrix, where each row represents a hotels avg attribute scores\n",
    "def findAvg(filename):\n",
    "    matrix = []\n",
    "    first = []\n",
    "    cell = []\n",
    "    keli = []\n",
    "    final = []\n",
    "    finalsec= []\n",
    "    names = []\n",
    "    hoteldata = open(filename, 'r')\n",
    "    k = 0\n",
    "    l = 0\n",
    "    dictionary = []\n",
    "    totalscore = 0\n",
    "    for line in hoteldata:\n",
    "        item = line.rstrip()\n",
    "        name = item.partition(\":\")[0]\n",
    "        rest = item.partition(\":\")[2]\n",
    "        attr = rest.partition(\":\")[0]\n",
    "        tser = rest.partition(\":\")[2]\n",
    "        rating = tser.partition(\":\")[0]\n",
    "        score = tser.partition(\":\")[2]\n",
    "        if attr != 'Business service (e.g., internet access)' and attr != 'Check in / front desk' and name != 'Element Boston Seaport':\n",
    "            if k == 0:\n",
    "                dictionary = [name, attr, (int(rating))*(int(score))]\n",
    "                totalscore = totalscore + (int(score))\n",
    "                k=k+1\n",
    "            elif name == eman :\n",
    "                if attr == rtta:\n",
    "                    dictionary[2] = dictionary[2]+((int(rating))*(int(score)))\n",
    "                    totalscore = totalscore + (int(score))\n",
    "                    k = k+1\n",
    "                if k == 5:\n",
    "                    dictionary[2] = ((float(dictionary[2]))/(float(totalscore)))\n",
    "                    totalscore = 0\n",
    "                    k=0\n",
    "                    l=l+1\n",
    "                    cell.append(dictionary)\n",
    "            if l == 6:\n",
    "                l = 0\n",
    "                keli.append(cell)\n",
    "            eman = item.partition(\":\")[0]\n",
    "            rtta = rest.partition(\":\")[0]\n",
    "    cell = sorted(cell, key=lambda tup: tup[1])\n",
    "    cell = sorted(cell, key=lambda tup: tup[0])\n",
    "    q=1\n",
    "    for i in range(0,len(cell)):\n",
    "        final.append(cell[q-1][2])\n",
    "        if q%6 == 0:\n",
    "            finalsec.append(final)\n",
    "            final=[]\n",
    "        q=q+1\n",
    "    finalsec = np.array(finalsec)\n",
    "    return finalsec.astype(float)\n",
    "\n",
    "X = (findAvg(\"/Users/johnsakk/Desktop/hotels.dat\"))\n",
    "Z=copy.deepcopy(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 2.984e+04\n",
      "Date:                Wed, 30 Mar 2016   Prob (F-statistic):          4.63e-123\n",
      "Time:                        01:10:23   Log-Likelihood:                 84.656\n",
      "No. Observations:                  80   AIC:                            -157.3\n",
      "Df Residuals:                      74   BIC:                            -143.0\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0388      0.100      0.388      0.699        -0.160     0.238\n",
      "x2             0.1196      0.032      3.777      0.000         0.057     0.183\n",
      "x3             0.5248      0.071      7.343      0.000         0.382     0.667\n",
      "x4             0.1888      0.084      2.258      0.027         0.022     0.355\n",
      "x5             0.0274      0.082      0.334      0.740        -0.136     0.191\n",
      "x6             0.0863      0.056      1.547      0.126        -0.025     0.197\n",
      "==============================================================================\n",
      "Omnibus:                        2.483   Durbin-Watson:                   1.836\n",
      "Prob(Omnibus):                  0.289   Jarque-Bera (JB):                1.736\n",
      "Skew:                          -0.151   Prob(JB):                        0.420\n",
      "Kurtosis:                       2.345   Cond. No.                         125.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Good news! R-squared is 1. x2 and x3 seem like the best predictors as P is 0. They should be kept for sure.\n",
    "# We could also keep x4. Last, AIC and BIC are very small.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.cluster.hierarchy as hr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.utils as utils\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.cross_validation as cross_validation\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "%matplotlib inline\n",
    "\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 3 (30 pts) **\n",
    "\n",
    "Finally, we will use logistic regression to decide if a hotel is _excellent_ or not. We classify a hotel as _excellent_ if more than **60%** of its ratings are 5 stars. This is a binary attribute on which we can fit a logistic regression model. As before, use the model to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.\n",
      "  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "  1.  1.  0.  0.  0.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#In this code block we compute the 80x1 binary vector where each hotels entry is a 1 if more than 60% of its ratings are excellent, 0 otherwise\n",
    "def getBinRating(w):\n",
    "    azure = copy.deepcopy(w)\n",
    "    q=0\n",
    "    for k in azure:\n",
    "        headers = { 'User-Agent' : user_agent }\n",
    "        response = requests.get(k[1], headers=headers)\n",
    "        htmlpage = response.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(htmlpage)\n",
    "        five = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_5'})\n",
    "        #print(five)\n",
    "        fifty = five.find_all('span')\n",
    "        fifty = fifty[2]\n",
    "        fifty = fifty.find(text=True)\n",
    "        #print (fifty)\n",
    "        four = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_4'})\n",
    "        forty = four.find_all('span')\n",
    "        forty = forty[2]\n",
    "        forty = forty.find(text=True)\n",
    "        #print (forty)\n",
    "        three = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_3'})\n",
    "        thirty = three.find_all('span')\n",
    "        thirty = thirty[2]\n",
    "        thirty = thirty.find(text=True)\n",
    "        #print (thirty)\n",
    "        two = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_2'})\n",
    "        twenty = two.find_all('span')\n",
    "        twenty=twenty[2]\n",
    "        twenty = twenty.find(text=True)\n",
    "        #print(twenty)\n",
    "        one = soup.find('label', {'for': 'taplc_prodp13n_hr_sur_review_filter_controls_0_filterRating_1'})\n",
    "        ten = one.find_all('span')\n",
    "        ten = ten[2]\n",
    "        ten = ten.find(text=True)\n",
    "        #print(ten)\n",
    "        if fifty == None:\n",
    "            fifty = \"0\"\n",
    "        if forty == None:\n",
    "            forty = \"0\"\n",
    "        if thirty == None:\n",
    "            thirty = '0'\n",
    "        if twenty == None:\n",
    "            twenty = '0'\n",
    "        if ten == None:\n",
    "            ten = '0'\n",
    "        binaryvalue = float(int(fifty.replace(',', '')))/float(int(fifty.replace(',', ''))+int(forty.replace(',', ''))+int(thirty.replace(',', ''))+int(twenty.replace(',', ''))+int(ten.replace(',', '')))   \n",
    "        if binaryvalue>0.6:\n",
    "            azure[q][1] = 1\n",
    "        else:\n",
    "            azure[q][1] = 0\n",
    "        q=q+1\n",
    "    azure = sorted(azure, key=lambda tup: tup[0])\n",
    "    azure = np.array(azure)\n",
    "    return(azure[:,1].astype(float))\n",
    "x = getBinRating(y)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.352329\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   80\n",
      "Model:                          Logit   Df Residuals:                       74\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 30 Mar 2016   Pseudo R-squ.:                  0.4127\n",
      "Time:                        01:21:17   Log-Likelihood:                -28.186\n",
      "converged:                       True   LL-Null:                       -47.992\n",
      "                                        LLR p-value:                 1.789e-07\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1           -22.9553      6.058     -3.789      0.000       -34.830   -11.081\n",
      "x2            -0.0335      1.261     -0.027      0.979        -2.505     2.438\n",
      "x3            21.4084      5.468      3.915      0.000        10.690    32.126\n",
      "x4             8.6464      4.297      2.012      0.044         0.223    17.069\n",
      "x5            -1.8385      3.162     -0.581      0.561        -8.036     4.359\n",
      "x6            -4.7443      3.033     -1.564      0.118       -10.688     1.200\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#As we see here, Log likelihood is very negative, which is good, pseudo R-squared has a pretty high score for the possibility of variance caused\n",
    "# by the binary nature of the y vector, the p value is really close to 0. x1 and x3 should be kept for certain. \n",
    "logit = sm.Logit(x,Z)\n",
    "result = logit.fit() \n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
